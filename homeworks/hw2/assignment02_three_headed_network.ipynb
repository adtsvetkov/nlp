{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13pL--6rycN3"
      },
      "source": [
        "## Homework02: Three headed network in PyTorch\n",
        "\n",
        "This notebook accompanies the [week02](https://github.com/girafe-ai/natural-language-processing/tree/master/week02_cnn_for_texts) practice session. Refer to that notebook for more comments.\n",
        "\n",
        "All the preprocessing is the same as in the classwork. *Including the data leakage in the train test split (it's still for bonus points).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P8zS7m-gycN5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyg2eIqbRHpP"
      },
      "source": [
        "If you have already downloaded the data on the Seminar, simply run through the next cells. Otherwise uncomment the next cell (and comment the another one ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPjv-BmGRHpQ",
        "outputId": "3154631f-7289-408c-a246-1b6457704e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   128    0   128    0     0    392      0 --:--:-- --:--:-- --:--:--   392\n",
            "100   342  100   342    0     0    543      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  119M  100  119M    0     0  47.4M      0  0:00:02  0:00:02 --:--:--  106M\n",
            "Train_rev1.csv\n",
            "--2023-03-06 23:05:49--  https://raw.githubusercontent.com/girafe-ai/natural-language-processing/22f_msai/homeworks/assignment02_three_headed_network/network.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1469 (1.4K) [text/plain]\n",
            "Saving to: ‘network.py.1’\n",
            "\n",
            "network.py.1        100%[===================>]   1.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-06 23:05:49 (16.4 MB/s) - ‘network.py.1’ saved [1469/1469]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# uncomment and run this cell, if you don't have data locally yet.\n",
        "\n",
        "!curl -L \"https://www.dropbox.com/s/5msc5ix7ndyba10/Train_rev1.csv.tar.gz?dl=1\" -o Train_rev1.csv.tar.gz\n",
        "!tar -xvzf ./Train_rev1.csv.tar.gz\n",
        "\n",
        "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/girafe-ai/natural-language-processing/22f_msai/homeworks/assignment02_three_headed_network/network.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwN72gd4ycOA"
      },
      "outputs": [],
      "source": [
        "# run this cell if you have downloaded the dataset on the seminar\n",
        "# data = pd.read_csv(\"../../week02_CNN_n_Vanishing_gradient/Train_rev1.csv\", index_col=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UuuKIKfrycOH"
      },
      "outputs": [],
      "source": [
        "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
        "text_columns = [\"Title\", \"FullDescription\"]\n",
        "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
        "target_column = \"Log1pSalary\"\n",
        "\n",
        "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast missing values to string \"NaN\"\n",
        "\n",
        "data.sample(3)\n",
        "\n",
        "\n",
        "data_for_autotest = data[-5000:]\n",
        "data = data[:-5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUWkpd7PycOQ",
        "outputId": "fd5f5beb-5b80-4011-8cba-112eabf937b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized:\n",
            "2         mathematical modeller / simulation analyst / o...\n",
            "100002    a successful and high achieving specialist sch...\n",
            "200002    web designer html , css , javascript , photosh...\n",
            "Name: FullDescription, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "239768it [00:49, 4878.30it/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "# see task above\n",
        "def normalize(text):\n",
        "    text = str(text).lower()\n",
        "    return ' '.join(tokenizer.tokenize(text))\n",
        "    \n",
        "data[text_columns] = data[text_columns].applymap(normalize)\n",
        "\n",
        "print(\"Tokenized:\")\n",
        "print(data[\"FullDescription\"][2::100000])\n",
        "assert data[\"FullDescription\"][2][:50] == 'mathematical modeller / simulation analyst / opera'\n",
        "assert data[\"Title\"][54321] == 'international digital account manager ( german )'\n",
        "\n",
        "# Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
        "# build a dictionary { token -> it's count }\n",
        "from collections import Counter\n",
        "from tqdm import tqdm as tqdm\n",
        "\n",
        "token_counts = Counter()# <YOUR CODE HERE>\n",
        "for _, row in tqdm(data[text_columns].iterrows()):\n",
        "    for string in row:\n",
        "        token_counts.update(string.split())\n",
        "\n",
        "# hint: you may or may not want to use collections.Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2fItdPwRHpU",
        "outputId": "854898d9-c79b-4864-d8e2-7b069c52937c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2598827"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "token_counts.most_common(1)[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiOWbc15ycOb",
        "outputId": "30814aca-0347-4195-edec-33f5d6b6ac5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique tokens : 201127\n",
            "('and', 2598827)\n",
            "('.', 2471477)\n",
            "(',', 2266256)\n",
            "('the', 2036428)\n",
            "('to', 1977039)\n",
            "...\n",
            "('dbms_stats', 1)\n",
            "('dbms_output', 1)\n",
            "('dbms_job', 1)\n",
            "Correct!\n",
            "Vocabulary size: 33795\n",
            "Correct!\n",
            "Correct!\n"
          ]
        }
      ],
      "source": [
        "print(\"Total unique tokens :\", len(token_counts))\n",
        "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
        "print('...')\n",
        "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
        "\n",
        "assert token_counts.most_common(1)[0][1] in  range(2500000, 2700000)\n",
        "assert len(token_counts) in range(200000, 210000)\n",
        "print('Correct!')\n",
        "\n",
        "min_count = 10\n",
        "\n",
        "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
        "tokens = [token for token, count in token_counts.items() if count >= min_count]# <YOUR CODE HERE>\n",
        "# Add a special tokens for unknown and empty words\n",
        "UNK, PAD = \"UNK\", \"PAD\"\n",
        "tokens = [UNK, PAD] + sorted(tokens)\n",
        "print(\"Vocabulary size:\", len(tokens))\n",
        "\n",
        "assert type(tokens) == list\n",
        "assert len(tokens) in range(32000, 35000)\n",
        "assert 'me' in tokens\n",
        "assert UNK in tokens\n",
        "print(\"Correct!\")\n",
        "\n",
        "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
        "assert isinstance(token_to_id, dict)\n",
        "assert len(token_to_id) == len(tokens)\n",
        "for tok in tokens:\n",
        "    assert tokens[token_to_id[tok]] == tok\n",
        "\n",
        "print(\"Correct!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JEsLeBjVycOw"
      },
      "outputs": [],
      "source": [
        "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
        "\n",
        "def as_matrix(sequences, max_len=None):\n",
        "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
        "    if isinstance(sequences[0], str):\n",
        "        sequences = list(map(str.split, sequences))\n",
        "        \n",
        "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
        "    \n",
        "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
        "    for i,seq in enumerate(sequences):\n",
        "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
        "        matrix[i, :len(row_ix)] = row_ix\n",
        "    \n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiBlPkdKycOy",
        "outputId": "c287dac4-666c-4d56-9755-1ad4c2748179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lines:\n",
            "engineering systems analyst\n",
            "hr assistant\n",
            "senior ec & i engineer\n",
            "\n",
            "Matrix:\n",
            "[[10705 29830  2143     1     1]\n",
            " [14875  2817     1     1     1]\n",
            " [27345 10107    15 15069 10702]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Lines:\")\n",
        "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
        "print(\"Matrix:\")\n",
        "print(as_matrix(data[\"Title\"][::100000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DpOlBp7ZycO6",
        "outputId": "5ed838aa-3a3c-4e98-e760-a4b80eb5a821"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DictVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, sparse=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, sparse=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "# we only consider top-1k most frequent companies to minimize memory usage\n",
        "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
        "recognized_companies = set(top_companies)\n",
        "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
        "\n",
        "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
        "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk4jmtAYycO8"
      },
      "source": [
        "### The deep learning part\n",
        "\n",
        "Once we've learned to tokenize the data, let's design a machine learning experiment.\n",
        "\n",
        "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
        "\n",
        "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes.\n",
        "\n",
        "\n",
        "#### Here comes the simple one-headed network from the seminar. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TngLcWA0ycO_",
        "outputId": "9d5464fe-0d2f-4d06-ea10-14dfe1862526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size =  191814\n",
            "Validation size =  47954\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
        "data_train.index = range(len(data_train))\n",
        "data_val.index = range(len(data_val))\n",
        "\n",
        "print(\"Train size = \", len(data_train))\n",
        "print(\"Validation size = \", len(data_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2PXuKgOSycPB"
      },
      "outputs": [],
      "source": [
        "def make_batch(data, max_len=None, word_dropout=0):\n",
        "    \"\"\"\n",
        "    Creates a keras-friendly dict from the batch data.\n",
        "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
        "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
        "    \"\"\"\n",
        "    batch = {}\n",
        "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len)\n",
        "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len)\n",
        "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
        "    \n",
        "    if word_dropout != 0:\n",
        "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
        "    \n",
        "    if target_column in data.columns:\n",
        "        batch[target_column] = data[target_column].values\n",
        "    \n",
        "    return batch\n",
        "\n",
        "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
        "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
        "    dropout_mask &= matrix != pad_ix\n",
        "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I6LpEQf0ycPD"
      },
      "outputs": [],
      "source": [
        "a = make_batch(data_train[:3], max_len=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNOf-ho_RHpa"
      },
      "source": [
        "But to start with let's build the simple model using only the part of the data. Let's create the baseline solution using only the description part (so it should definetely fit into the Sequential model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5dcqu95BRHpa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mWWHd6pmRHpa"
      },
      "outputs": [],
      "source": [
        "# You will need these to make it simple\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class Reorder(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.permute((0, 2, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRjMhS1URHpb"
      },
      "source": [
        "To generate minibatches we will use simple pyton generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cOrqZNANRHpb"
      },
      "outputs": [],
      "source": [
        "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, **kwargs):\n",
        "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
        "    while True:\n",
        "        indices = np.arange(len(data))\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(indices)\n",
        "\n",
        "        for start in range(0, len(indices), batch_size):\n",
        "            batch = make_batch(data.iloc[indices[start : start + batch_size]], **kwargs)\n",
        "            target = batch.pop(target_column)\n",
        "            yield batch, target\n",
        "        \n",
        "        if not cycle: break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "68t4aatkx7zb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "caTHv1ZtRHpb"
      },
      "outputs": [],
      "source": [
        "iterator = iterate_minibatches(data_train, 3)\n",
        "batch, target = next(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FcKHRtVqRHpc"
      },
      "outputs": [],
      "source": [
        "# Here is some startup code:\n",
        "n_tokens=len(tokens)\n",
        "n_cat_features=len(categorical_vectorizer.vocabulary_)\n",
        "hid_size=64\n",
        "simple_model = nn.Sequential()\n",
        "\n",
        "simple_model.add_module('emb', nn.Embedding(num_embeddings=n_tokens, embedding_dim=hid_size))\n",
        "simple_model.add_module('reorder', Reorder())\n",
        "simple_model.add_module('conv1', nn.Conv1d(\n",
        "    in_channels=hid_size,\n",
        "    out_channels=hid_size,\n",
        "    kernel_size=2)\n",
        "                       )\n",
        "simple_model.add_module('relu1', nn.ReLU())\n",
        "simple_model.add_module('adapt_avg_pool', nn.AdaptiveAvgPool1d(output_size=1))\n",
        "simple_model.add_module('flatten1', Flatten())\n",
        "simple_model.add_module('linear1', nn.Linear(in_features=hid_size, out_features=1))\n",
        "# <YOUR CODE HERE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ_1w2BGRHpc",
        "outputId": "0a9c2a9c-cc2d-4c07-d8bd-231743c89589"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Title': array([[24093, 18670,   195, 24093, 17567,   195, 16522, 30080, 17567,\n",
              "           195, 17649],\n",
              "        [12095,  2166, 23148,  2143,     1,     1,     1,     1,     1,\n",
              "             1,     1],\n",
              "        [11665, 18670,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1]], dtype=int32),\n",
              " 'FullDescription': array([[24093, 18670,   195, 24093, 17567,   195, 16522, 30080, 17567,\n",
              "           195, 17649,   195, 31823, 30762,    80, 16873, 17041, 33304,\n",
              "         24093, 18670,   195, 24093, 17567,   195, 16522, 30080, 17567,\n",
              "           195, 17649,   195, 18704,   195, 14935,   195, 32907, 33628,\n",
              "         14938, 18704, 16289, 23213, 30762,  2237, 30411, 25112, 12466,\n",
              "           965, 24093, 18670,   195, 24093, 17567,   195, 16522, 30080,\n",
              "         17567, 33198,   965, 14446, 24043,  3238, 10568, 15402, 30411,\n",
              "         32907, 33628,  2546, 33010, 16289, 18235, 30762,  7786,   965,\n",
              "          5722, 21405, 11295, 22048, 30411, 20752, 31438, 33591,  2166,\n",
              "         15476, 30422, 14188,  7626, 30276, 12413,   167,  2662,  2120,\n",
              "         11458, 24093, 18670,   195, 24093, 17567,   195, 16522, 30080,\n",
              "         17567, 33635, 33079, 14109, 11453, 21405, 10327, 28294,  9000,\n",
              "         17864,   195, 17437, 26907, 10792,   195, 14446, 31070, 29830,\n",
              "           156,  7217,  2166, 19596,   156, 33198, 30411,  1041, 30762,\n",
              "         18659,  2166,  1563, 28828,  2166,  2529, 21556, 30411,  8894,\n",
              "          2166,  9000, 21405, 13360, 24114,   167,     0, 30512, 16289,\n",
              "           965, 17438, 16522,  9000, 30080,   156, 11562, 11453, 15402,\n",
              "         16522, 16325, 11068,   167, 30411, 29406, 17567, 33079, 14109,\n",
              "         11453, 21405, 30080, 18664,   156, 33331, 33198, 17041, 28294,\n",
              "          9000, 30103, 30578, 30411, 12850, 27079,  2166, 15143, 23876,\n",
              "           437, 24558,   167, 33635,    18, 18065,  3607, 10817,   156,\n",
              "           965, 13404, 13251,  2166, 16969, 30762, 24079, 15402, 30512,\n",
              "         31719, 13360,  4938,   167, 15187, 33635, 18961, 30411,  1059,\n",
              "           156, 23212, 27339, 33642,  8167, 15294, 21784,  9230, 30762,\n",
              "         29000,   167,     0,   167,  6681,   167, 30411, 26682, 21556,\n",
              "         21416, 16289,    80,    80, 33198, 11562,  3771,   167, 30411,\n",
              "          2545, 18130, 15402,  2120, 15142, 18132, 12466,  6822, 12769,\n",
              "         33628,   156,  8854,  2166, 13598, 18704,   167, 17041, 33304,\n",
              "         24093, 18670,   195, 24093, 17567,   195, 16522, 30080, 17567,\n",
              "           195, 17649,   195, 18704,   195, 14935,   195, 32907, 33628,\n",
              "         23212, 21089, 33635, 33079, 25001,  2120,  3106, 25855,  1568,\n",
              "         33635, 30407, 32718, 14109, 25002, 33642,  8167,   167, 14938,\n",
              "         16289,   965, 17576, 24249, 21405, 22759, 25112,   156,  7338,\n",
              "         24027,  2166, 29913, 18664, 28350, 33383,   167],\n",
              "        [21972,  6347,   156,   965, 17576, 25930, 15402,  5714, 18192,\n",
              "         27196,   965, 12095,  2166, 23148,  2143, 30762, 16729, 30411,\n",
              "          4938, 12466,   759, 19947,    32, 30512,  7614,  1973, 13404,\n",
              "         22759,    71, 30512, 31993,  1405, 27836, 32033, 30762, 30411,\n",
              "         32694, 30411,  4938, 23153,  2166, 12483, 16416, 12105,  2166,\n",
              "          6739, 22720,   167, 30411, 26324, 10618, 30411, 24257, 21405,\n",
              "          4938,  2166,  6739,  2142, 30762, 10725, 24051,  2166, 10268,\n",
              "          2166, 10600,  8552, 18615, 33198,   965, 22365, 12396, 21556,\n",
              "          7594,   156,  5531, 22720,  2166, 18815, 18664,   167, 30512,\n",
              "          2143, 33079,  3607, 25722, 30762, 15940, 33198,  1894,  2548,\n",
              "         21405, 30411,  4938,   156, 22366, 30411,  4477,  4978,  1973,\n",
              "         25927,   156, 12095,  2166, 16378, 10601, 11804,  1308,  2166,\n",
              "          8556, 29566, 30578, 30411, 24257, 21405,  2142,  2166, 23148,\n",
              "         25058,   167, 21084, 21595, 33079, 30411, 26324, 29894, 22129,\n",
              "         21405, 30411, 32802, 12482,  2166,  4811,  4978, 33079,  1973,\n",
              "         28960, 30411,  6835, 21556,  8944, 12105,  2142,  2166,  8341,\n",
              "         16027,   195, 18741, 30762, 10778, 30407, 12105, 23148, 16289,\n",
              "          6305,   156, 24922, 30654,  2166,  1220,   167, 12916,   156,\n",
              "         32718, 20573, 28367, 30407, 14083,   965, 22405,  2166,  2120,\n",
              "          2371, 12466,  9792, 16079, 28685,  2166, 16033, 26377,  5573,\n",
              "          2166,  9809, 30762,  8704,  1220,  2166, 32828, 15656,   167,\n",
              "         15187, 33635,  2545, 15294,  3137,  2166,  2545, 16969, 30762,\n",
              "          3607,  7197,   156, 23212, 27339, 33642,  8949, 33214,  8675,\n",
              "           167,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1],\n",
              "        [ 3512,  2892,     0,  6649,   156, 14059,   156, 21058, 33628,\n",
              "         26682,  3384, 13841,    80, 32718,  2545, 27195, 30762,  2414,\n",
              "           965, 23936,  2166, 11458, 17569, 30762, 18659, 30411,  5538,\n",
              "          2166,  6296,   195, 17487, 21689,  2892, 21972, 25792,  6649,\n",
              "          5192, 21556,     0, 17412,   156, 14059,   167, 32718, 14109,\n",
              "         25792,  1150, 12466,    80,  9259, 29257,  2166, 24244, 31823,\n",
              "         30762,    80, 19168, 22697,  8422,   167, 17819, 33198, 30411,\n",
              "         18594, 30080,   156, 30411, 29406, 22789, 33079,  1973,  3607,\n",
              "         16184, 33198, 30411, 12214, 12466, 21972, 20722,  4841, 19217,\n",
              "          2166, 16378,  5722,   167, 30512, 16289,   965, 32364, 26324,\n",
              "         30407, 10780, 30407,  1894, 21972, 11665, 21682,     0, 15402,\n",
              "         21809, 30762, 10778, 21972, 29257,   156,  4353, 25792,  2166,\n",
              "          8422,   156,  5196, 17605, 15402,   965, 26643,  2166, 27175,\n",
              "         10866,   167, 30411, 29406, 22789, 33079, 14109, 24556,  2166,\n",
              "           195, 21784, 11562, 11453, 15402,  5538,   195, 11665, 18664,\n",
              "          2166, 18700, 20230, 30103,   167, 30411, 23445,  6414, 21556,\n",
              "         30411,   432, 11910,   461,  2892,    80, 21034,   167, 12466,\n",
              "           965, 16658,  8879,  2166,   965, 22789, 28553,   156, 32335,\n",
              "         21972, 32773,   167, 32718, 32817,  2389, 12769,  1894, 27168,\n",
              "         21405, 30411,  6813,   167, 30411, 29406, 23445, 14634, 33079,\n",
              "          3607, 29312, 30762,  2120, 10726,  7777,  9294,  5947,   167,\n",
              "         30512, 16658, 32637, 21870, 23459,  2662, 33468,   167, 30895,\n",
              "           167,  6681,   195, 16679,   195, 11666,    80,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1]], dtype=int32),\n",
              " 'Categorical': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_6hko1lRHpd"
      },
      "source": [
        "__Remember!__ We are working with regression problem and predicting only one number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrzQ4ysiRHpd",
        "outputId": "7a7df41c-d5a3-4cb4-ddca-22a2d4e79e1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1297],\n",
              "        [-0.2570],\n",
              "        [-0.2755]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Try this to check your model. `torch.long` tensors are required for nn.Embedding layers.\n",
        "simple_model(torch.tensor(batch['FullDescription'], dtype=torch.long))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oczI2P5xRHpd",
        "outputId": "9dd4f9e8-8439-4412-d762-acf709730345"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 304)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "batch['FullDescription'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojuqr1RxRHpe"
      },
      "source": [
        "And now simple training pipeline (it's commented because we've already done that in class. No need to do it again)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "v_u3BqNtRHpe",
        "outputId": "7eb07a2d-538c-4530-fe5d-3fd288173d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "model = simple_model\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "history = []\n",
        "for epoch_num in range(epochs):\n",
        "  for idx, (batch, target) in enumerate(iterate_minibatches(data_train)):\n",
        "    # Preprocessing the batch data and target\n",
        "    batch = torch.tensor(batch['FullDescription'], dtype=torch.long)\n",
        "\n",
        "    target = torch.tensor(target)\n",
        "    predictions = model(batch)\n",
        "    predictions = predictions.view(predictions.size(0))\n",
        "    \n",
        "    loss = loss_func(predictions, target)# <YOUR CODE HERE>\n",
        "\n",
        "    # train with backprop\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    history.append(loss.data.numpy())\n",
        "    if (idx+1)%10==0:\n",
        "      clear_output(True)\n",
        "      plt.plot(history,label='loss')\n",
        "      plt.legend()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# дополнительно посмотрим что у нас по качеству:\n",
        "\n",
        "squared_error = abs_error = num_samples = 0.0\n",
        "\n",
        "# предиктить будем по батчам, чтобы ядро не умирало\n",
        "for batch, target in iterate_minibatches(data_val, shuffle=False):\n",
        "    batch = torch.tensor(batch['FullDescription'], dtype=torch.long)\n",
        "\n",
        "    predictions = model(batch)[:, 0].detach().numpy()\n",
        "\n",
        "    squared_error += np.sum(np.square(predictions - target))\n",
        "    abs_error += np.sum(np.abs(predictions - target))\n",
        "    num_samples += len(target)\n",
        "\n",
        "print(\"Mean square error: %.5f\" % (squared_error / num_samples))\n",
        "print(\"Mean absolute error: %.5f\" % (abs_error / num_samples))"
      ],
      "metadata": {
        "id": "V5c4cdq8Ruzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff46d01-e79c-425a-a9be-d8ac1cb46dd7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean square error: 0.17227\n",
            "Mean absolute error: 0.32360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU61eXc0RHpf"
      },
      "source": [
        "### Actual homework starts here\n",
        "__Your ultimate task is to code the three headed network described on the picture below.__ \n",
        "To make it closer to the real world, please store the network code in file `network.py` in this directory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eI5h9UMycPF"
      },
      "source": [
        "#### Architecture\n",
        "\n",
        "Our main model consists of three branches:\n",
        "* Title encoder\n",
        "* Description encoder\n",
        "* Categorical features encoder\n",
        "\n",
        "We will then feed all 3 branches into one common network that predicts salary.\n",
        "\n",
        "<img src=\"https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png\" width=600px>\n",
        "\n",
        "This clearly doesn't fit into PyTorch __Sequential__ interface. To build such a network, one will have to use [__PyTorch nn.Module API__](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bViC8Jb8RHpf"
      },
      "outputs": [],
      "source": [
        "import network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Yp7YJq6LRHpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78f7f41-2bdb-4ae9-cfb1-d3b3e2246475"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'network' from '/content/network.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Re-run this cell if you updated the file with network source code\n",
        "import imp\n",
        "imp.reload(network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RZO2RwvmRHpg"
      },
      "outputs": [],
      "source": [
        "model = network.ThreeInputsNet(\n",
        "    n_tokens=len(tokens),\n",
        "    n_cat_features=len(categorical_vectorizer.vocabulary_),\n",
        "\n",
        "    # this parameter defines the number of the inputs in the layer,\n",
        "    # which stands after the concatenation. In should be found out by you.\n",
        "    concat_number_of_features = 64*3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "1pC61iPpRHpg"
      },
      "outputs": [],
      "source": [
        "testing_batch, _ = next(iterate_minibatches(data_train, 3))\n",
        "testing_batch = [\n",
        "    torch.tensor(testing_batch['Title'], dtype=torch.long),\n",
        "    torch.tensor(testing_batch['FullDescription'], dtype=torch.long),\n",
        "    torch.tensor(testing_batch['Categorical'])\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bk7wME3RRHph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6c2568-378e-43bd-bf88-49690a134b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems fine!\n"
          ]
        }
      ],
      "source": [
        "assert model(testing_batch).shape == torch.Size([3, 1])\n",
        "assert model(testing_batch).dtype == torch.float32\n",
        "print('Seems fine!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMioZQcZRHph"
      },
      "source": [
        "Now train the network for a while (100 batches would be fine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "KfZ_pc9cRHph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "6ba6bd1d-d8da-4a17-f454-1eafecd331ed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Training pipeline comes here (almost the same as for the simple_model)\n",
        "\n",
        "epochs = 1\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "history = []\n",
        "\n",
        "for epoch_num in range(epochs):\n",
        "  model.train()\n",
        "  for idx, (batch, target) in enumerate(iterate_minibatches(data_train)):\n",
        "    # Preprocessing the batch data and target\n",
        "    batch = [torch.tensor(batch['Title'], dtype=torch.long),\n",
        "             torch.tensor(batch['FullDescription'], dtype=torch.long),\n",
        "             torch.tensor(batch['Categorical'])]\n",
        "\n",
        "    target = torch.tensor(target)\n",
        "    predictions = model(batch)\n",
        "    predictions = predictions.view(predictions.size(0))\n",
        "    \n",
        "    loss = loss_func(predictions, target)\n",
        "\n",
        "    # train with backprop\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    history.append(loss.data.numpy())\n",
        "    if (idx+1)%10==0:\n",
        "      clear_output(True)\n",
        "      plt.plot(history,label='loss')\n",
        "      plt.legend()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EenXIizPRHpi"
      },
      "source": [
        "Now, to evaluate the model it can be switched to `eval` state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "H0dkoYW0RHpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ea07e0-02b7-4871-d819-9627cf890fe1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ThreeInputsNet(\n",
              "  (title_emb): Embedding(33795, 64)\n",
              "  (title_conv): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n",
              "  (title_pool): AdaptiveAvgPool1d(output_size=1)\n",
              "  (full_emb): Embedding(33795, 64)\n",
              "  (full_conv): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n",
              "  (full_pool): AdaptiveMaxPool1d(output_size=1)\n",
              "  (category_out): Linear(in_features=3746, out_features=64, bias=True)\n",
              "  (dropout): Dropout(p=0.35, inplace=False)\n",
              "  (inter_dense): Linear(in_features=192, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (final_dense): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0Uwe4gWIRHpi"
      },
      "outputs": [],
      "source": [
        "def generate_submission(model, data, batch_size=256, name=\"\", three_inputs_mode=True, **kw):\n",
        "    squared_error = abs_error = num_samples = 0.0\n",
        "    output_list = []\n",
        "    for batch_x, batch_y in tqdm(iterate_minibatches(data, batch_size=batch_size, shuffle=False, **kw)):\n",
        "        if three_inputs_mode:\n",
        "            batch = [\n",
        "                torch.tensor(batch_x['Title'], dtype=torch.long),\n",
        "                torch.tensor(batch_x['FullDescription'], dtype=torch.long),\n",
        "                torch.tensor(batch_x['Categorical'])\n",
        "            ]\n",
        "        else:\n",
        "            batch = torch.tensor(batch_x['FullDescription'], dtype=torch.long)\n",
        "\n",
        "        batch_pred = model(batch)[:, 0].detach().numpy()\n",
        "        \n",
        "        output_list.append((list(batch_pred), list(batch_y)))\n",
        "        \n",
        "        squared_error += np.sum(np.square(batch_pred - batch_y))\n",
        "        abs_error += np.sum(np.abs(batch_pred - batch_y))\n",
        "        num_samples += len(batch_y)\n",
        "    print(\"%s results:\" % (name or \"\"))\n",
        "    print(\"Mean square error: %.5f\" % (squared_error / num_samples))\n",
        "    print(\"Mean absolute error: %.5f\" % (abs_error / num_samples))\n",
        "    \n",
        "\n",
        "    batch_pred = [c for x in output_list for c in x[0]]\n",
        "    batch_y = [c for x in output_list for c in x[1]]\n",
        "    output_df = pd.DataFrame(list(zip(batch_pred, batch_y)), columns=['batch_pred', 'batch_y'])\n",
        "    output_df.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "DOB6tXS8RHpj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a6eb3f-0cb1-4f76-c858-6978d775c58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [00:06,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission results:\n",
            "Mean square error: 0.16436\n",
            "Mean absolute error: 0.31238\n",
            "Submission file generated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "generate_submission(model, data_for_autotest, name='Submission')\n",
        "print('Submission file generated')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Качество на 3 бранчах получилось лучше, чем на 1. Ура!!"
      ],
      "metadata": {
        "id": "zdT3tTS9ZqEQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGXyx2nORHpj"
      },
      "source": [
        "__Both the notebook and the `.py` file are required to submit this homework.__"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}